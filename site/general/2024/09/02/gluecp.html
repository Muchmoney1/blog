

<!DOCTYPE html>
<html>
<meta charset="UTF-8">
<style>
@media (prefers-color-scheme: dark) {
    body {
        background-color: #1c1c1c;
        color: white;
    }
    .markdown-body table tr {
        background-color: #1c1c1c;
    }
    .markdown-body table tr:nth-child(2n) {
        background-color: black;
    }
}
</style>



<link rel="alternate" type="application/rss+xml" href="../../../../feed.xml" title="Glue and coprocessor architectures">



<link rel="stylesheet" type="text/css" href="../../../../css/main.css">

<script type="text/x-mathjax-config">
<script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\(', '\)']]
  },
  svg: {
    fontCache: 'global',
  }
};
</script>
<script type="text/javascript" id="MathJax-script" async
  src="../../../../scripts/tex-svg.js">
</script>

<style>
</style>

<div id="doc" class="container-fluid markdown-body comment-enabled" data-hard-breaks="true">

<div id="color-mode-switch">
  <svg xmlns="http://www.w3.org/2000/svg" class="h-6 w-6" fill="none" viewBox="0 0 24 24" stroke="currentColor" stroke-width="2">
    <path stroke-linecap="round" stroke-linejoin="round" d="M12 3v1m0 16v1m9-9h-1M4 12H3m15.364 6.364l-.707-.707M6.343 6.343l-.707-.707m12.728 0l-.707.707M6.343 17.657l-.707.707M16 12a4 4 0 11-8 0 4 4 0 018 0z" />
  </svg>
  <input type="checkbox" id="switch" />
  <label for="switch">Dark Mode Toggle</label>
  <svg xmlns="http://www.w3.org/2000/svg" class="h-6 w-6" fill="none" viewBox="0 0 24 24" stroke="currentColor" stroke-width="2">
    <path stroke-linecap="round" stroke-linejoin="round" d="M20.354 15.354A9 9 0 018.646 3.646 9.003 9.003 0 0012 21a9.003 9.003 0 008.354-5.646z" />
  </svg>
</div>

<script type="text/javascript">
  // Update root html class to set CSS colors
  const toggleDarkMode = () => {
    const root = document.querySelector('html');
    root.classList.toggle('dark');
  }

  // Update local storage value for colorScheme
  const toggleColorScheme = () => {
    const colorScheme = localStorage.getItem('colorScheme');
    if (colorScheme === 'light') localStorage.setItem('colorScheme', 'dark');
    else localStorage.setItem('colorScheme', 'light');
  }

  // Set toggle input handler
  const toggle = document.querySelector('#color-mode-switch input[type="checkbox"]');
  if (toggle) toggle.onclick = () => {
    toggleDarkMode();
    toggleColorScheme();
  }

  // Check for color scheme on init
  const checkColorScheme = () => {
    const colorScheme = localStorage.getItem('colorScheme');
    // Default to light for first view
    if (colorScheme === null || colorScheme === undefined) localStorage.setItem('colorScheme', 'light');
    // If previously saved to dark, toggle switch and update colors
    if (colorScheme === 'dark') {
      toggle.checked = true;
      toggleDarkMode();
    }
  }
  checkColorScheme();
</script>

<meta name="twitter:card" content="summary" />
<meta name="twitter:title" content="Glue and coprocessor architectures" />
<meta name="twitter:image" content="http://vitalik.ca/images/icon.png" />


<br>
<h1 style="margin-bottom:7px"> Glue and coprocessor architectures </h1>
<small style="float:left; color: #888"> 2024 Sep 02 </small>
<small style="float:right; color: #888"><a href="../../../../index.html">See all posts</a></small>
<br> <br> <br>
<title> Glue and coprocessor architectures </title>

<p><em>Special thanks to Justin Drake, Georgios Konstantopoulos, Andrej
Karpathy, Michael Gao, Tarun Chitra and various Flashbots contributors
for feedback and review.</em></p>
<p>If you analyze any resource-intensive computation being done in the
modern world in even a medium amount of detail, one feature that you
will find again and again is that the computation can be broken up into
two parts:</p>
<ol type="1">
<li>A relatively small amount of complex, but not very computationally
intensive, "<strong>business logic</strong>"</li>
<li>A large amount of intensive, but highly structured,
"<strong>expensive work</strong>"</li>
</ol>
<p>These two forms of computation are best handled in different ways:
the former, with an architecture that may have lower efficiency but
needs to have very high generality, and the latter, with an architecture
that may have lower generality, but needs to have very high
efficiency.</p>
<h2 id="what-are-some-examples-of-this-separation-in-practice">What are
some examples of this separation in practice?</h2>
<p>To start off, let us look under the hood of the environment I am most
familiar with: the <strong>Ethereum Virtual Machine (EVM)</strong>. <a
href="https://etherscan.io/vmtrace?txhash=0x38cab69c76d8d7069f8b90e170a853740f6370f975de9f4d90f50ed6adc2e6ee">Here
is the geth debug trace</a> of a recent Ethereum transaction that I did:
updating the IPFS hash of my blog on ENS. The transaction consumes a
total of 46924 gas, which can be categorized in this way:</p>
<ul>
<li><strong>Base cost</strong>: 21,000</li>
<li><strong>Calldata</strong>: 1,556</li>
<li><strong>EVM execution</strong>: 24,368
<ul>
<li><strong><code>SLOAD</code></strong> opcode: 6,400</li>
<li><strong><code>SSTORE</code></strong> opcode: 10,100</li>
<li><strong><code>LOG</code></strong> opcode: 2,149</li>
<li><strong>Other</strong>: 6,719</li>
</ul></li>
</ul>
<center>
<p><br></p>
<p><img src="../../../../images/gluecp/HyZWi1qjA.png" /></p>
<p><small></p>
<p><em>EVM trace of an ENS hash update. Second last column is gas
consumption.</em></p>
</small>
</center>
<p><br></p>
<p>The moral of the story is: <strong>most of the execution</strong>
(~73% if you look at the EVM alone, ~85% if you include the portion of
the base cost that covers computation) is concentrated in a <strong>very
small number of structured expensive operations</strong>: storage reads
and writes, logs, and cryptography (the base cost includes 3000 to pay
for signature verification, and the EVM also includes 272 to pay for
hashing). <strong>The rest of the execution is "business
logic"</strong>: fiddling around with the bits of the calldata to
extract the ID of the record I am trying to set and the hash I am
setting it to, and so on. In a token transfer, this would include adding
and subtracting balances, in a more advanced application, this might
include a loop, and so on.</p>
<p>In the EVM, these two forms of execution are handled in different
ways. The high-level business logic is written in a higher-level
language, often Solidity, which compiles to the EVM. The expensive work
is still <em>triggered</em> by EVM opcodes (<code>SLOAD</code>, etc),
but &gt; 99% of the actual computation is done in specialized modules
written directly inside of client code (or even libraries).</p>
<p>To reinforce our understanding of this pattern, let's explore it in
another context: AI code written in python using <a
href="https://pypi.org/project/torch/">torch</a>.</p>
<center>
<p><br></p>
<p><img src="../../../../images/gluecp/SkguXlcsR.png" /></p>
<p><small></p>
<p><em>Forward pass of one block of a transformer model, <a
href="https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/transformers/transformer_flux.py#L81">source</a>.</em></p>
</small>
</center>
<p><br></p>
<p>What do we see here? We see a relatively small amount of "business
logic", written in python, which describes the <em>structure</em> of the
operations that are being done. In an actual application, there will
also be another type of business logic, which determines details like
how you get the input and what you do to the output. But, if we peek
into each of the individual operations themselves
(<code>self.norm</code>, <code>torch.cat</code>, <code>+</code>,
<code>*</code>, the various steps inside <code>self.attn</code>...), we
see <em>vectorized</em> computation: the same operation getting computed
on a large number of values in parallel. Similarly to the first example,
a small portion of the compute is spent on business logic, and the bulk
of the compute is spent on performing the big structured matrix and
vector operations - in fact, the majority is just matrix
multiplication.</p>
<p>Just like in the EVM example, the two types of work are handled in
two different ways. The high-level business logic code is written in
Python, a highly general and flexible language which is also very slow,
and we just accept the inefficiency because it only touches a small part
of the total computational cost. Meanwhile, the intensive operations are
written in highly optimized code, often CUDA code running on a GPU.
Increasingly, we're even starting to see LLM inference <a
href="https://mobile.x.com/marek_rosa/status/1820163725703204957">being
done on ASICs</a>.</p>
<p><strong>Modern <a
href="https://0xparc.org/blog/programmable-cryptography-1">programmable
cryptography</a>, such as <a
href="https://vitalik.eth.limo/general/2021/01/26/snarks.html">SNARKs</a>,
follows a similar pattern yet again, <em>on two levels</em></strong>.
First, the prover can be written in a high-level language where the
heavy work is done with vectorized operations, just like the AI example
above. My <a
href="https://github.com/ethereum/research/blob/master/circlestark/fast_fft.py">circle
STARK code here</a> shows this in action. Second, the program that is
being executed inside the cryptography can itself be written in a way
that is split between generalized business logic and highly structured
expensive work.</p>
<p>To see how this works, we can look at one of the latest trends in
STARK proving. To be general-purpose and easy to use, teams are
increasingly building STARK provers for widely-adopted minimal virtual
machines, such as <a
href="https://en.wikipedia.org/wiki/RISC-V">RISC-V</a>. Any program
whose execution needs to be proven can be compiled into RISC-V, and then
the prover can prove the RISC-V execution of that code.</p>
<center>
<p><br></p>
<p><img
src="../../../../images/gluecp/from-rust-to-receipt-23117368c4f46d78c8cac3b753245a5a.png" /></p>
<p><small></p>
<p><em>Diagram from <a
href="https://dev.risczero.com/api/zkvm/">RiscZero
documentation</a></em></p>
</small>
</center>
<p><br></p>
<p>This is super convenient: it means that we only need to write the
prover logic once, and from that point forward any program that needs to
be proven can just be written in any "conventional" programming language
(eg. RiskZero supports Rust). However, there is a problem: this approach
incurs significant overhead. Programmable cryptography is already very
expensive; adding the overhead of running code inside a RISC-V
interpreter is too much. And so developers have come up with a hack: you
identify the specific expensive operations that make up the bulk of the
computation (often that's hashes and signatures), and you create
specialized modules to prove those operations extremely efficiently. And
then you just combine the inefficient-but-general RISC-V proving system
and the efficient-but-specialized proving systems together, and you get
the best of both worlds.</p>
<p>Programmable cryptography other than ZK-SNARKs, such as <a
href="https://en.wikipedia.org/wiki/Secure_multi-party_computation">multi-party
computation (MPC)</a> and <a
href="https://vitalik.eth.limo/general/2020/07/20/homomorphic.html">fully
homomorphic encryption (FHE)</a> will likely be optimized using a
similar approach.</p>
<h2 id="what-is-the-general-pattern-at-play">What is the general pattern
at play?</h2>
<p>Modern computation is increasingly following what I call a
<strong>glue and coprocessor</strong> architecture: you have some
central "glue" component, which has high generality but low efficiency,
which is responsible for shuttling data between one or more
<strong>coprocessor</strong> components, which have low generality but
high efficiency.</p>
<center>
<p><br></p>
<p><img src="../../../../images/gluecp/BJdoaxqo0.png" /></p>
</center>
<p><br></p>
<p>This is a simplification: in practice, there are almost always more
than two levels along the tradeoff curve between efficiency and
generality. GPUs, and other chips that are often called "coprocessors"
in industry, are less general than CPUs but more general than ASICs.
There are complicated tradeoffs of <em>how far</em> to specialize, which
are decided based on projections and intuitions about what parts of an
algorithm will still be the same in five years, and which parts will
change in six months. In a ZK-proving architecture, we often similarly
see multiple layers of specialization. But for a broad mental model,
it's sufficient to think about two levels. There are parallels to this
in many domains of computation:</p>
<p><br></p>
<table>
<colgroup>
<col style="width: 33%" />
<col style="width: 33%" />
<col style="width: 33%" />
</colgroup>
<thead>
<tr class="header">
<th>Domain</th>
<th>Glue</th>
<th>Coprocessor</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Ethereum</td>
<td>EVM</td>
<td>Dedicated opcodes/precompiles for specialized operations</td>
</tr>
<tr class="even">
<td>AI (eg. LLMs)</td>
<td>Python (often)</td>
<td>GPU via CUDA; ASICs</td>
</tr>
<tr class="odd">
<td>Web apps</td>
<td>Javascript</td>
<td>WASM</td>
</tr>
<tr class="even">
<td>Programmable cryptography</td>
<td>RISC-V</td>
<td>Specialized modules (eg. for hashes and signatures)</td>
</tr>
</tbody>
</table>
<p><br></p>
<p>From the above examples, it might feel like a law of nature that of
course computation can be split in this way. And indeed, you can find
examples of specialization in computing for decades. However, I would
argue that <strong>this separation is increasing</strong>. I think this
is true for a few key reasons:</p>
<ol type="1">
<li>We have only relatively recently hit the limits of increasing CPU
clock speed, and so further gains can only come from
<strong>parallelization</strong>. However, parallelization is hard to
reason about, and so it's often more practical for developers to
continue reasoning sequentially, and let the parallelization happen in
the backend, wrapped inside specialized modules built for specific
operations.</li>
<li>Computation has only recently become so fast that the
<strong>computational costs of business logic have become truly
negligible</strong>. In this world, it makes sense to optimize the VM
that business logic runs in for goals other than compute efficiency:
developer friendliness, familiarity, security, and other similar
objectives. Meanwhile, the specialized "coprocessor" modules can
continue to be designed for efficiency, and get their security and
developer friendliness properties from the relatively simple "interface"
that they have with the glue.</li>
<li><strong>It's becoming clearer what the most important expensive
operations are</strong>. This is most obvious in cryptography, where
it's clear what kinds of specific expensive operations are most likely
to be used: modular arithmetic, elliptic curve <a
href="https://ethresear.ch/t/simple-guide-to-fast-linear-combinations-aka-multiexponentiations/7238">linear
combinations</a> (aka multi-scalar multiplications), Fast Fourier
transforms, and so on. It's also becoming clearer in AI, where the bulk
of computation has been "mostly matrix multiplication" (albeit with <a
href="https://medium.com/@techresearchspace/what-is-quantization-in-llm-01ba61968a51">different</a>
levels of <a href="https://arxiv.org/abs/2402.17764">precision</a>) for
over two decades. Similar trends appear in other domains. There are just
much fewer unknown unknowns in (computationally intensive) computing
than there were 20 years ago.</li>
</ol>
<h2 id="what-does-this-imply">What does this imply?</h2>
<p>A key takeaway is that <strong>glue should optimize for being good
glue, and coprocessors should optimize for being good
coprocessors</strong>. We can explore the implications of this in a few
key areas.</p>
<h3 id="evm">EVM</h3>
<p><strong>Blockchain virtual machines (eg. EVM) don't need to be
efficient, they just need to be familiar.</strong> Computation in an
inefficient VM can be made almost as efficient in practice as
computation in a natively efficient VM by just adding the right
coprocessors (aka "precompiles"). The overhead incurred by eg. the EVM's
256-bit registers is relatively small, while the benefits from the EVM's
familiarity and existing developer ecosystem are great and durable.
Developer teams optimizing the EVM are even finding that lack of
parallelization is often not a primary barrier to scalability.</p>
<p>The best ways to improve the EVM may well just be (i) <strong>adding
better precompiles or specialized opcodes</strong>, eg. some combination
of <a
href="https://ethereum-magicians.org/t/eip-6601-evm-modular-arithmetic-extensions-evmmax/13168">EVM-MAX</a>
and <a href="https://eips.ethereum.org/EIPS/eip-616">SIMD</a> may be
justified, and (ii) <strong>improving the storage layout</strong>, which
eg. the <a href="https://eips.ethereum.org/EIPS/eip-4762">Verkle tree
changes</a> do as a side effect by greatly reducing the cost of
accessing storage slots that are beside each other.</p>
<center>
<p><br></p>
<p><img src="../../../../images/gluecp/verkle.png" /></p>
<p><small></p>
<p><em>Storage optimizations in the Ethereum Verkle tree proposal,
putting adjacent storage keys together and adjusting gas costs to
reflect this. Optimizations like this, together with better precompiles,
may well matter more than tweaking the EVM itself.</em></p>
</small>
</center>
<p><br></p>
<h3 id="secure-computing-and-open-hardware">Secure computing and open
hardware</h3>
<p>One of the big challenges with improving security of modern computing
at the hardware layer is the overcomplicated and proprietary nature of
it: chips are designed to be highly efficient, which requires
proprietary optimizations. Backdoors are easy to hide, and <a
href="https://www.securityweek.com/nearly-all-modern-cpus-leak-data-to-new-collidepower-side-channel-attack/">side
channel vulnerabilities</a> keep getting discovered.</p>
<p>There continues to be a valiant effort to push more open and more
secure alternatives from multiple angles. Some computations are
increasingly done in trusted execution environments, including on users'
phones, and this has increased security for users already. The push
toward more-open-source consumer hardware continues, with recent
victories like a <a
href="https://www.omgubuntu.co.uk/2024/06/the-worlds-first-risc-v-laptop-running-ubuntu">RISC-V
laptop running Ubuntu</a>.</p>
<center>
<p><br></p>
<p><img src="../../../../images/gluecp/SJl7jMJnA.png" /></p>
<p><small></p>
<p><em>RISC-V laptop running Debian, <a
href="https://www.youtube.com/watch?v=vzod1yUzPhc">source</a></em></p>
</small>
</center>
<p><br></p>
<p>However, efficiency continues to be a problem. The author of the
above-linked article writes:</p>
<blockquote>
<p>It's unfeasible for a newer, open-source chip design like RISC-V to
go toe to toe with processor technologies that have been around for and
refined for decades. Progress has a starting point.</p>
</blockquote>
<p>More paranoid ideas, like this <a
href="https://www.contrib.andrew.cmu.edu/~somlo/BTCP/">design for
building a RISC-V computer on top of an FPGA</a>, face even more
overhead. <strong>But what if glue and coprocessor architectures mean
that this overhead does not actually matter</strong>? What if we accept
that open and secure chips will be be slower than proprietary chips, if
needed even giving up on common optimizations like speculative execution
and branch prediction, but try to compensate for this by adding (if
needed, proprietary) ASIC modules for specific types of computation that
are the most intensive? Sensitive computations can be done in a "main
chip" that would be optimized for security, open source design and
side-channel resistance. More intensive computations (eg. ZK-proving,
AI) would be done in the ASIC modules, which would learn less
information (potentially, with cryptographic blinding, perhaps in some
cases even zero information) about the computation being performed.</p>
<h3 id="cryptography">Cryptography</h3>
<p>Another key takeaway is that this is all very optimistic for
cryptography, especially <a
href="https://0xparc.org/blog/programmable-cryptography-1">programmable
cryptography</a>, going mainstream. We're already seeing hyper-optimized
implementations of some specific highly structured computations in
SNARKs, MPC and other settings: overhead for <a
href="https://starkware.co/blog/starkware-new-proving-record">some hash
functions</a> is in the range of being only a few hundred times more
expensive than running the computation directly, and extremely low
overhead for AI (which is mostly just matrix multiplications) is <a
href="https://vitalik.eth.limo/general/2024/01/30/cryptoai.html#cryptographic-overhead">also
possible</a>. Further improvements like <a
href="https://people.cs.georgetown.edu/jthaler/GKRNote.pdf">GKR</a> will
likely reduce this further. Fully general purpose VM execution,
especially if executed inside a RISC-V interpreter, will likely continue
to have something like ten-thousand-fold overhead, but for the reasons
described in this post, <em>this will not matter</em>: as long as the
most intensive parts of a computation are handled separately using
efficient dedicated techniques, the total overhead will be
manageable.</p>
<center>
<p><br></p>
<p><img src="../../../../images/gluecp/mpcnets.png" /></p>
<p><small></p>
<p><em>A simplified diagram of a dedicated MPC for matrix
multiplication, the largest component in AI model inference. See <a
href="https://eprint.iacr.org/2018/403.pdf">this paper</a> for more
details, including ways to keep both the model and the input
private.</em></p>
</small>
</center>
<p><br></p>
<p>One exception to the idea that "glue only needs to be familiar, not
efficient" is <em>latency</em>, and to a smaller extent <em>data
bandwidth</em>. If a computation involves doing repeated heavy
operations on the same data dozens of times (as cryptography and AI both
do), any delays that result from an inefficient glue layer can become a
primary bottleneck to running time. Hence, glue also has efficiency
requirements, though they are more specific ones.</p>
<h3 id="conclusion">Conclusion</h3>
<p>On the whole, I consider the above trends to be very positive
developments from several perspectives. First, it is the logical way to
maximize computing efficiency while preserving developer friendliness,
and being able to get more of both at the same time benefits everyone.
In particular, by enabling more efficiency gains from specialization on
the client side, it improves our ability to run computations that are
both sensitive and performance-demanding (eg. ZK-proving, LLM inference)
locally on the user's hardware. Second, <strong>it creates a large
window of opportunity to ensure that the drive for efficiency does not
compromise other values, most notably security, openness and
simplicity</strong>: side-channel security and openness in computer
hardware, reducing circuit complexity in ZK-SNARKs, and reducing
complexity in virtual machines. Historically, the drive for efficiency
has led to these other factors taking a back seat. With
glue-and-coprocessor architectures, <em>it no longer needs to</em>. One
part of the machine optimizes for efficiency, and the other part
optimizes for generality and other values, and the two work
together.</p>
<p>The trend is also very beneficial for cryptography, because
cryptography itself is a major example of "expensive structured
computation", which gets accelerated by this trend. This adds a further
opportunity to increase security. A security increase also becomes
possible in the world of blockchains: we can worry less about optimizing
virtual machines, and instead focus more on optimizing precompiles and
other features that live alongside virtual machines.</p>
<p>Third, <strong>this trend presents an opportunity for smaller and
newer players to participate</strong>. If computation is becoming less
monolithic, and more modular, that greatly decreases the barrier to
entry. Even with an ASIC for one type of computation, it's possible to
make a difference. The same will be true in the ZK-proving space, and in
EVM optimization. Writing code that has near-frontier-level efficiency
becomes much easier and more accessible. <em>Auditing and formally
verifying</em> such code becomes easier and more accessible. And
finally, because these very different domains of computing are
converging on some common patterns, there is more room for collaboration
and learning between them.</p>
 </div> 